{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/zhouxibin/models\")\n",
    "model_name = \"esm1v_t33_650M_UR90S_1\"\n",
    "data_path = Path(\"data\")\n",
    "dataset_name = \"B3VI55_LIPST_Whitehead2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wt_and_data(data_path, dataset_name):\n",
    "    \"\"\"load wildtype and data\n",
    "\n",
    "    Args:\n",
    "        data_path (str or pathlib.Path): Data path\n",
    "        dataset_name (str): dataset name\n",
    "\n",
    "    Returns:\n",
    "        fasta (Bio.SeqRecord): sequence\n",
    "        mut_fitness (pd.DataFrame): dataframe\n",
    "    \"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    fasta_dataset_path = data_path / dataset_name / \"{}.fasta\".format(dataset_name)\n",
    "    mut_fitness_dataset_path = data_path / dataset_name / \"{}.csv\".format(dataset_name)\n",
    "    \n",
    "    fasta = SeqIO.read(fasta_dataset_path, \"fasta\")\n",
    "    mut_fitness = pd.read_csv(mut_fitness_dataset_path)\n",
    "    return fasta, mut_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_row(row, sequence, token_probs, alphabet, offset_idx):\n",
    "    # print(row)\n",
    "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
    "    # print(idx, len(sequence))\n",
    "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence, idx={}, sequence[idx]={}, wt={}\".format(idx, sequence[idx], wt)\n",
    "\n",
    "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
    "\n",
    "    # add 1 for BOS\n",
    "    # print(token_probs.shape)\n",
    "    score = token_probs[0, 1 + idx, mt_encoded] - token_probs[0, 1 + idx, wt_encoded]\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pppl(row, sequence, model, alphabet, offset_idx):\n",
    "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
    "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence\"\n",
    "\n",
    "    # modify the sequence\n",
    "    sequence = sequence[:idx] + mt + sequence[(idx + 1) :]\n",
    "\n",
    "    # encode the sequence\n",
    "    data = [\n",
    "        (\"protein1\", sequence),\n",
    "    ]\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
    "\n",
    "    # compute probabilities at each position\n",
    "    log_probs = []\n",
    "    for i in range(1, len(sequence) - 1):\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        batch_tokens_masked[0, i] = alphabet.mask_idx\n",
    "        with torch.no_grad():\n",
    "            token_probs = torch.log_softmax(model(batch_tokens_masked.cuda())[\"logits\"], dim=-1)\n",
    "        log_probs.append(token_probs[0, i, alphabet.get_idx(sequence[i])].item())  # vocab size\n",
    "    return sum(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集的特殊情况\n",
    "- POLG_HCVJF_Sun2014: wildtype长度超过1022\n",
    "- PTEN_HUMAN_Fowler2018: 最后有一项是WT，需要手动在数据集里面删掉\n",
    "- PABP_YEAST_Fields2013-doubles: 双点突变，暂时处理不了\n",
    "- BRCA1_HUMAN_RING: wildtype长度超过1022\n",
    "- MTH3_HAEAESTABILIZED_Tawfik2015: 第26号位置csv中是A，而sequence中是C。并且由于这个是Stabilized，所以我们相信csv中的标记，将sequence改为A；同理104号将sequence的I改为K；115 M到L；181 F到L；327 C到R\n",
    "- UBC9_HUMAN_Roth2017: csv中159号位置有一个Y，但是sequence中没有，所以给sequence最后加入一个Y\n",
    "- UBE4B_MOUSE_Klevit2013-singles: 长度超过1022\n",
    "- TIM_THEMA_b0: 102号位置 C换成S\n",
    "- F7YBW7_MESOW_vae: 四点突变，暂时不处理\n",
    "- B3VI55_LIPSTSTABLE: 140号位置L变成I，142号位置从S变成A，373号位置A变成C\n",
    "- HIS7_YEAST_Kondrashov2017: 多点同时突变，暂时不处理\n",
    "- BRCA1_HUMAN_BRCT: 长度超过1022\n",
    "- TPMT_HUMAN_Fowler2018: 最后有一项是WT，需要手动在数据集里面删掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_datasets = [\n",
    "    \"POLG_HCVJF_Sun2014\", \"PABP_YEAST_Fields2013-doubles\", \"BRCA1_HUMAN_RING\", \n",
    "    \"UBE4B_MOUSE_Klevit2013-singles\", \"F7YBW7_MESOW_vae\", \"HIS7_YEAST_Kondrashov2017\",\n",
    "    \"BRCA1_HUMAN_BRCT\"\n",
    "]\n",
    "def score_fitness(model_path, model_name, data_path):\n",
    "    esm_path = model_path / \"{}.pt\".format(model_name)\n",
    "    model, alphabet = esm.pretrained.load_model_and_alphabet(str(esm_path.absolute()))\n",
    "    \n",
    "    scoring_strategy_pool = [\"wt-marginals\", \"masked-marginals\", \"pseudo-ppl\"]\n",
    "    scoring_strategy = scoring_strategy_pool[0]\n",
    "    \n",
    "    total = {}\n",
    "    for scoring_strategy in scoring_strategy_pool:\n",
    "        total[scoring_strategy] = {}\n",
    "        for dataset in data_path.glob(\"*\"):\n",
    "            dataset_name = dataset.stem\n",
    "            if dataset_name in wrong_datasets or not dataset.is_dir():\n",
    "                continue\n",
    "            # print(scoring_strategy, dataset_name)\n",
    "            wildtype, mut_fitness = score_fitness_one_iteration(model, alphabet, data_path, dataset_name, scoring_strategy)\n",
    "            total[scoring_strategy][dataset_name] = {\"wildtype\": str(wildtype.seq), \"mut_fitness\": mut_fitness}\n",
    "            spearmanr_ = spearmanr(mut_fitness.iloc[:, 1], mut_fitness.iloc[:, 2], nan_policy=\"omit\")\n",
    "            total[scoring_strategy][dataset_name][\"spearmanr\"] = spearmanr_.correlation\n",
    "            # break\n",
    "        break\n",
    "    return total\n",
    "\n",
    "def score_fitness_one_iteration(model, alphabet, data_path, dataset_name, scoring_strategy):\n",
    "    wildtype, mut_fitness = load_wt_and_data(data_path, dataset_name)\n",
    "    \n",
    "    if len(str(wildtype.seq)) > 1022:\n",
    "        print(\"wild type size is over 1022\")\n",
    "        return None, None\n",
    "    \n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"Transferred model to GPU\")\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    data = [\n",
    "        (\"protein1\", str(wildtype.seq)),\n",
    "    ]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    # print(len(wildtype.seq), batch_tokens.shape, len(batch_strs[0]), batch_labels[0])\n",
    "\n",
    "    offset_idx=1 # 生物中都是1开始的索引\n",
    "    if scoring_strategy == \"wt-marginals\":\n",
    "        with torch.no_grad():\n",
    "            token_probs = torch.log_softmax(model(batch_tokens.cuda())[\"logits\"], dim=-1)\n",
    "            # print(token_probs.shape)\n",
    "        mut_fitness[model_name+\"_\"+scoring_strategy] = mut_fitness.apply(\n",
    "            lambda row: label_row(\n",
    "                row[0],\n",
    "                str(wildtype.seq),\n",
    "                token_probs,\n",
    "                alphabet,\n",
    "                offset_idx,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    elif scoring_strategy == \"masked-marginals\":\n",
    "        all_token_probs = []\n",
    "        for i in tqdm(range(batch_tokens.size(1))):\n",
    "            batch_tokens_masked = batch_tokens.clone()\n",
    "            batch_tokens_masked[0, i] = alphabet.mask_idx\n",
    "            with torch.no_grad():\n",
    "                token_probs = torch.log_softmax(\n",
    "                    model(batch_tokens_masked.cuda())[\"logits\"], dim=-1\n",
    "                )\n",
    "            all_token_probs.append(token_probs[:, i])  # vocab size\n",
    "        token_probs = torch.cat(all_token_probs, dim=0).unsqueeze(0)\n",
    "        mut_fitness[model_name+\"_\"+scoring_strategy] = mut_fitness.apply(\n",
    "            lambda row: label_row(\n",
    "                row[0],\n",
    "                str(wildtype.seq),\n",
    "                token_probs,\n",
    "                alphabet,\n",
    "                offset_idx,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    elif scoring_strategy == \"pseudo-ppl\":\n",
    "        tqdm.pandas()\n",
    "        mut_fitness[model_name+\"_\"+scoring_strategy] = mut_fitness.progress_apply(\n",
    "            lambda row: compute_pppl(\n",
    "                row[0], str(wildtype.seq), model, alphabet, offset_idx\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    return wildtype, mut_fitness\n",
    "\n",
    "total = score_fitness(model_path, model_name, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dms-evaluate': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0765ee94399b90a847b07ed994827faa90e4e22b659722e9114690514c933a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
